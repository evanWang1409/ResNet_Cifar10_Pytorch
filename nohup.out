Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Sequential
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
LambdaLayer
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Sequential
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
LambdaLayer
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Sequential
Linear
ResNet
train_resnet_cifar10.py:25: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.
  init.kaiming_normal(m.weight)
[1, 1000] loss: 2.158
[1, 2000] loss: 1.850
[1, 3000] loss: 1.720
[1, 4000] loss: 1.656
[1, 5000] loss: 1.599
[1, 6000] loss: 1.522
[1, 7000] loss: 1.455
[1, 8000] loss: 1.407
[2, 1000] loss: 1.295
[2, 2000] loss: 1.285
[2, 3000] loss: 1.227
[2, 4000] loss: 1.218
[2, 5000] loss: 1.192
[2, 6000] loss: 1.150
[2, 7000] loss: 1.148
[2, 8000] loss: 1.122
[3, 1000] loss: 1.017
[3, 2000] loss: 0.988
[3, 3000] loss: 1.008
[3, 4000] loss: 0.965
[3, 5000] loss: 0.944
[3, 6000] loss: 0.949
[3, 7000] loss: 0.958
[3, 8000] loss: 0.913
[4, 1000] loss: 0.818
[4, 2000] loss: 0.842
[4, 3000] loss: 0.829
[4, 4000] loss: 0.820
[4, 5000] loss: 0.807
[4, 6000] loss: 0.817
[4, 7000] loss: 0.803
[4, 8000] loss: 0.787
[5, 1000] loss: 0.723
[5, 2000] loss: 0.698
[5, 3000] loss: 0.699
[5, 4000] loss: 0.698
[5, 5000] loss: 0.693
[5, 6000] loss: 0.697
[5, 7000] loss: 0.700
[5, 8000] loss: 0.680
[6, 1000] loss: 0.584
[6, 2000] loss: 0.617
[6, 3000] loss: 0.615
[6, 4000] loss: 0.618
[6, 5000] loss: 0.620
[6, 6000] loss: 0.586
[6, 7000] loss: 0.624
[6, 8000] loss: 0.619
[7, 1000] loss: 0.501
[7, 2000] loss: 0.538
[7, 3000] loss: 0.553
[7, 4000] loss: 0.535
[7, 5000] loss: 0.547
[7, 6000] loss: 0.562
[7, 7000] loss: 0.519
[7, 8000] loss: 0.529
[8, 1000] loss: 0.460
[8, 2000] loss: 0.463
[8, 3000] loss: 0.459
[8, 4000] loss: 0.477
[8, 5000] loss: 0.481
[8, 6000] loss: 0.494
[8, 7000] loss: 0.471
[8, 8000] loss: 0.482
[9, 1000] loss: 0.421
[9, 2000] loss: 0.393
[9, 3000] loss: 0.396
[9, 4000] loss: 0.417
[9, 5000] loss: 0.425
[9, 6000] loss: 0.424
[9, 7000] loss: 0.440
[9, 8000] loss: 0.432
[10, 1000] loss: 0.338
[10, 2000] loss: 0.368
[10, 3000] loss: 0.377
[10, 4000] loss: 0.372
[10, 5000] loss: 0.382
[10, 6000] loss: 0.368
[10, 7000] loss: 0.365
[10, 8000] loss: 0.397
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Sequential
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
LambdaLayer
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Sequential
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
LambdaLayer
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Conv2d
BatchNorm2d
Conv2d
BatchNorm2d
Sequential
BasicBlock
Sequential
Linear
ResNet
train_resnet_cifar10.py:25: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.
  init.kaiming_normal(m.weight)
[1, 1000] loss: 2.052
[1, 2000] loss: 1.830
[1, 3000] loss: 1.714
[1, 4000] loss: 1.664
[1, 5000] loss: 1.579
[1, 6000] loss: 1.528
[1, 7000] loss: 1.465
[1, 8000] loss: 1.408
[2, 1000] loss: 1.303
[2, 2000] loss: 1.270
[2, 3000] loss: 1.263
[2, 4000] loss: 1.210
[2, 5000] loss: 1.183
[2, 6000] loss: 1.128
[2, 7000] loss: 1.124
[2, 8000] loss: 1.106
[3, 1000] loss: 1.034
[3, 2000] loss: 0.997
[3, 3000] loss: 0.999
[3, 4000] loss: 0.991
[3, 5000] loss: 0.960
[3, 6000] loss: 0.913
[3, 7000] loss: 0.923
[3, 8000] loss: 0.904
[4, 1000] loss: 0.850
[4, 2000] loss: 0.840
[4, 3000] loss: 0.812
[4, 4000] loss: 0.834
[4, 5000] loss: 0.832
[4, 6000] loss: 0.799
[4, 7000] loss: 0.798
[4, 8000] loss: 0.793
[5, 1000] loss: 0.729
[5, 2000] loss: 0.732
[5, 3000] loss: 0.708
[5, 4000] loss: 0.702
[5, 5000] loss: 0.710
[5, 6000] loss: 0.711
[5, 7000] loss: 0.714
[5, 8000] loss: 0.714
[6, 1000] loss: 0.635
[6, 2000] loss: 0.649
[6, 3000] loss: 0.626
[6, 4000] loss: 0.656
[6, 5000] loss: 0.643
[6, 6000] loss: 0.653
[6, 7000] loss: 0.640
[6, 8000] loss: 0.618
[7, 1000] loss: 0.575
[7, 2000] loss: 0.559
[7, 3000] loss: 0.598
[7, 4000] loss: 0.561
[7, 5000] loss: 0.564
[7, 6000] loss: 0.562
[7, 7000] loss: 0.576
[7, 8000] loss: 0.562
[8, 1000] loss: 0.487
[8, 2000] loss: 0.518
[8, 3000] loss: 0.519
[8, 4000] loss: 0.526
[8, 5000] loss: 0.523
[8, 6000] loss: 0.519
[8, 7000] loss: 0.538
[8, 8000] loss: 0.528
[9, 1000] loss: 0.454
[9, 2000] loss: 0.462
[9, 3000] loss: 0.476
[9, 4000] loss: 0.472
[9, 5000] loss: 0.468
[9, 6000] loss: 0.492
[9, 7000] loss: 0.460
[9, 8000] loss: 0.486
[10, 1000] loss: 0.398
[10, 2000] loss: 0.411
[10, 3000] loss: 0.426
[10, 4000] loss: 0.440
[10, 5000] loss: 0.430
[10, 6000] loss: 0.440
[10, 7000] loss: 0.424
[10, 8000] loss: 0.451
